1.  实验是在连网的环境下进行的
2.  CPU训练较慢，当train时设置“max_steps”尽量不要太大，可以设为1000（训练时间为15-20分钟）
3.  batch_size：Batch的选择，首先决定的是下降的方向。实验中，我们将以 batch_size 为最小批量来处理数据。Batch_Size 太小，数据集不收敛。随着 Batch_Size 增大，处理相同数据量的速度越快，但达到相同精度所需要的 epoch 数量越来越多，花费的时间也大大增加，从而对参数的修正就显得更加缓慢。
4.  learning_rate：学习率控制了每次更新参数的幅度，如果学习率太大就会导致更新的幅度太大，就有可能会跨过损失值的极小值（不说最小值的原因，是因为有可能是局部的最小值），最后可能参数的极优值会在参数的极优值之间徘徊，而取不到参数的极优值。如果学习率设置的太小，就会导致更新速度太慢，从而需要消耗更多的资源来保证获取到参数的极优值。
5.  num_steps：即time_step，表示单个序列的长度，也就是输入的词的个数
6.  num_seqs：表示每个batch中序列的个数
7.  num_layers：表示需要的lstm层数
8.  lstm_size：表示lstm隐藏层规模
9.  use_embedding：表示是否需要embedding层，中文等字符较多，需要embedding，否则矩阵过于稀疏
10. embedding_size：表示embedding层的规模